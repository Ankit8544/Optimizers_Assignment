{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 1: Understanding Optimizers**\n",
    "\n",
    "1. **What is the role of optimization algorithms in artificial neural networks? Why are they necessary?**\n",
    "\n",
    "2. **Explain the concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms of convergence speed and memory requirements.**\n",
    "\n",
    "3. **Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow convergence, local minima). How do modern optimizers address these challenges?**\n",
    "\n",
    "4. **Discuss the concepts of momentum and learning rate in the context of optimization algorithms. How do they impact convergence and model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Role of Optimization Algorithms in Artificial Neural Networks**\n",
    "\n",
    "   - **Role -** Optimization algorithms play a crucial role in artificial neural networks by minimizing the loss function, which quantifies the difference between the predicted output and the actual target values. The optimization process adjusts the weights and biases in the network to reduce this error, effectively \"learning\" from the data to improve model performance.\n",
    "\n",
    "   - **Necessity -** These algorithms are necessary because neural networks contain millions of parameters that need to be fine-tuned to ensure accurate predictions. Manual tuning is impractical, and optimization algorithms provide a systematic approach to find the best set of parameters.\n",
    "\n",
    "2. **Gradient Descent and its Variants**\n",
    "\n",
    "   - **Gradient Descent (GD) -** This is a first-order optimization algorithm that iteratively updates the model's parameters by moving in the direction of the negative gradient of the loss function with respect to the parameters. The step size is determined by the learning rate. GD is classified as:\n",
    "\n",
    "     - **Batch Gradient Descent :** Computes the gradient using the entire dataset at once. This ensures a stable convergence but can be slow and memory-intensive for large datasets.\n",
    "\n",
    "     - **Stochastic Gradient Descent (SGD) :** Computes the gradient for each data point individually. It is much faster and requires less memory, but the parameter updates are noisier, leading to less stable convergence.\n",
    "\n",
    "     - **Mini-batch Gradient Descent :** Combines the benefits of both batch and stochastic methods by calculating gradients on small random batches of data, offering a balance between speed and stability.\n",
    "\n",
    "   - **Variants of Gradient Descent -**\n",
    "\n",
    "     - **Momentum :** Accelerates gradient descent by adding a fraction of the previous update to the current one, helping to avoid oscillations and speeding up convergence.\n",
    "\n",
    "     - **AdaGrad :** Adapts the learning rate for each parameter by scaling it based on past gradients, which helps with sparse data but can lead to overly small learning rates.\n",
    "\n",
    "     - **RMSProp :** Modifies AdaGrad by using a moving average of squared gradients to scale the learning rate, preventing it from becoming too small.\n",
    "\n",
    "     - **Adam (Adaptive Moment Estimation) :** Combines momentum and RMSProp by using moving averages of both the gradients and the squared gradients, making it a popular choice for various tasks due to its efficiency in convergence.\n",
    "\n",
    "   - **Tradeoffs -**\n",
    "\n",
    "     - **Convergence Speed :** SGD is typically faster than batch GD, but less stable. Adam converges faster than vanilla SGD but can sometimes overfit. Momentum-based methods help avoid slow convergence in plateaus.\n",
    "\n",
    "     - **Memory Requirements :** Batch gradient descent requires more memory since it processes the entire dataset at once, while mini-batch and stochastic approaches are more memory-efficient.\n",
    "\n",
    "3. **Challenges with Traditional Gradient Descent**\n",
    "\n",
    "   - **Slow Convergence :** Especially in deep networks, gradient descent may converge slowly, particularly when the landscape of the loss function contains flat regions or plateaus.\n",
    "\n",
    "   - **Local Minima and Saddle Points :** Traditional gradient descent can get stuck in local minima or saddle points (regions where the gradient is zero but not necessarily a minimum), leading to suboptimal solutions.\n",
    "\n",
    "   - **Learning Rate Sensitivity :** A poorly chosen learning rate can result in either slow convergence or overshooting the minimum, causing the model to never converge.\n",
    "   \n",
    "   **Modern Optimizers' Solutions -**\n",
    "\n",
    "   - **Momentum-Based Methods :** By introducing momentum, these methods help smooth out the trajectory of the optimization, avoiding oscillations and speeding up convergence.\n",
    "\n",
    "   - **Adaptive Learning Rates :** Algorithms like Adam, RMSProp, and AdaGrad adjust the learning rate dynamically based on past gradients, allowing the optimization to adapt to different regions of the loss function and improving convergence speed.\n",
    "\n",
    "   - **Escape from Saddle Points :** Modern optimizers like Adam and momentum-based methods add \"inertia\" to the gradient updates, helping the optimizer escape flat regions and saddle points.\n",
    "\n",
    "4. **Momentum and Learning Rate**\n",
    "\n",
    "   - **Momentum -**\n",
    "\n",
    "     - **Definition :** Momentum adds a fraction of the previous update vector to the current update, allowing the optimizer to maintain directionality and build velocity. This helps accelerate the optimization process, especially in directions with consistently steep gradients.\n",
    "\n",
    "     - **Impact on Convergence :** Momentum can reduce oscillations in directions of noisy gradients and speeds up convergence in flatter directions. It is particularly useful for overcoming the challenges of local minima and saddle points.\n",
    "   \n",
    "   - **Learning Rate -**\n",
    "\n",
    "     - **Definition :** The learning rate controls the step size at each iteration while updating the model parameters. A higher learning rate allows for larger updates, while a lower learning rate results in smaller updates.\n",
    "     \n",
    "     - **Impact on Convergence :** A too-large learning rate can cause the optimizer to overshoot the minimum, resulting in divergence, while a too-small learning rate can lead to slow convergence and getting stuck in suboptimal solutions.\n",
    "    \n",
    "     - **Learning Rate Scheduling:** Techniques like learning rate decay, cyclic learning rates, or adaptive learning rates can be employed to adjust the learning rate over time, balancing speed and stability during training.\n",
    "\n",
    "`In summary`, optimization algorithms, particularly gradient descent and its variants, play a pivotal role in fine-tuning neural network parameters. Modern methods address challenges like slow convergence and local minima by using techniques such as momentum and adaptive learning rates to accelerate convergence and enhance performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Optimizer Techniques**\n",
    "\n",
    "5. **Explain the concept of Stochastic Gradient Descent (SGD) and its advantages compared to traditional gradient descent. Discuss its limitations and scenarios where it is most suitable.**\n",
    "\n",
    "6. **Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates. Discuss its benefits and potential drawbacks.**\n",
    "\n",
    "7. **Explain the concept of RMSprop optimizer and how it addresses the challenges of adaptive learning rates. Compare it with Adam and discuss their relative strengths and weaknesses.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "   - **Concept -** Stochastic Gradient Descent is a variant of gradient descent where the model's parameters are updated for each training sample rather than the entire dataset. Instead of computing the gradient using the whole dataset (as in batch gradient descent), SGD approximates the gradient by using a single randomly selected training example or a small batch.\n",
    "\n",
    "   - **Advantages of SGD -**\n",
    "\n",
    "     - **Speed and Efficiency :** Since SGD updates the model parameters after evaluating only one data point or a small batch, it is computationally faster and can process large datasets more efficiently, especially when the dataset doesn't fit into memory.\n",
    "   \n",
    "     - **Frequent Updates :** Due to the more frequent updates, SGD can escape local minima more easily and has a better chance of finding a global minimum. It allows the model to converge faster, at least in the early stages of training.\n",
    "   \n",
    "     - **Online Learning :** SGD supports online learning, making it suitable for real-time applications where data arrives in a stream rather than all at once.\n",
    "\n",
    "   - **Limitations of SGD -**\n",
    "\n",
    "     - **High Variance in Updates :** Because the updates are based on individual data points, they can introduce a high variance in the gradient estimates, leading to noisy parameter updates and unstable convergence.\n",
    "   \n",
    "     - **Less Stable Convergence :** The noisy updates can cause the optimization to oscillate around the minimum, resulting in slower or suboptimal convergence unless carefully tuned with techniques like learning rate scheduling or momentum.\n",
    "   \n",
    "   - **Suitable Scenarios -**\n",
    "\n",
    "     - **Large Datasets :** SGD is ideal for large datasets where full-batch gradient descent is computationally expensive.\n",
    "\n",
    "     - **Online Learning :** When data is received incrementally, SGD is the preferred method since it can update the model continuously with new data.\n",
    "\n",
    "     - **Sparse Data :** SGD works well for sparse data scenarios, such as natural language processing or recommendation systems, where data may not require full-batch processing.\n",
    "\n",
    "6. **Adam Optimizer**\n",
    "\n",
    "   - **Concept -** Adam (Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of both momentum and adaptive learning rates. It computes individual learning rates for each parameter by maintaining both a running average of the gradient (first moment) and the squared gradient (second moment).\n",
    "\n",
    "   - **How it Works -**\n",
    "\n",
    "     - **Momentum Component :** Adam uses the concept of momentum by keeping track of the exponentially decaying average of past gradients. This allows Adam to have a smoother trajectory, similar to the benefits of momentum, reducing oscillations during training.\n",
    "\n",
    "     - **Adaptive Learning Rate Component :** Adam also maintains an exponentially decaying average of the squared gradients (similar to RMSprop), which adapts the learning rate for each parameter independently. This helps the optimizer adjust the learning rate based on how steep or flat the gradient is for each parameter.\n",
    "\n",
    "   - **Benefits of Adam -**\n",
    "\n",
    "     - **Fast Convergence :** Adam tends to converge faster than traditional gradient-based optimizers, especially in complex neural networks with large amounts of data. Its momentum component accelerates the optimization, while the adaptive learning rate ensures efficient updates.\n",
    "\n",
    "     - **Works Well with Noisy Data :** The adaptive learning rate helps Adam perform well even with noisy gradients, reducing the need for fine-tuning the learning rate.\n",
    "\n",
    "     - **Good Generalization :** Adam has shown to generalize well across a wide range of problems and architectures, making it the go-to optimizer for many deep learning applications.\n",
    "\n",
    "   - **Potential Drawbacks -**\n",
    "\n",
    "     - **Overfitting :** Due to its adaptive learning rate, Adam may overfit in some scenarios where more careful tuning of the learning rate is required.\n",
    "\n",
    "     - **Lack of Convergence to the True Minimum :** Adam may not always converge to the global minimum and can sometimes stop early at suboptimal solutions, particularly in non-convex problems.\n",
    "     \n",
    "     - **Parameter Sensitivity :** Adam introduces additional hyperparameters (e.g., \\(\\beta_1\\), \\(\\beta_2\\)), which can make tuning more complex, especially when compared to simpler methods like SGD.\n",
    "\n",
    "7. **RMSprop Optimizer**\n",
    "\n",
    "   - **Concept -** RMSprop (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm designed to address the challenges posed by decaying learning rates over time (as seen in AdaGrad). RMSprop scales the learning rate for each parameter based on the moving average of squared gradients over recent iterations.\n",
    "\n",
    "   - **How it Works -**\n",
    "\n",
    "     - **Adaptive Learning Rates :** RMSprop maintains a moving average of the squared gradients and normalizes the gradients by this average. This allows it to adapt the learning rate for each parameter based on the recent history of updates, enabling more effective updates in regions with flat gradients.\n",
    "\n",
    "     - **Decay Factor :** The moving average introduces a decay factor (often set to around 0.9), ensuring that older gradients are progressively weighted less, preventing the learning rate from diminishing too quickly.\n",
    "\n",
    "   - **Strengths of RMSprop -**\n",
    "\n",
    "     - **Efficient with Non-stationary Data :** RMSprop adapts well to non-stationary objectives, making it effective for tasks like recurrent neural networks and reinforcement learning, where data distribution changes over time.\n",
    "\n",
    "     - **Stability in Training :** By adjusting learning rates based on the magnitude of gradients, RMSprop provides a more stable convergence compared to vanilla SGD. This makes it less likely to overshoot the minima and improves stability across epochs.\n",
    "\n",
    "   - **Weaknesses of RMSprop -**\n",
    "\n",
    "     - **Sensitive to Hyperparameters :** Like Adam, RMSprop's performance is sensitive to hyperparameter choices, particularly the decay rate and initial learning rate, which need to be tuned for optimal performance.\n",
    "   \n",
    "     - **Suboptimal Convergence :** While RMSprop stabilizes training and handles different learning rates across parameters, it may not always converge as efficiently as methods that use momentum (such as Adam).\n",
    "\n",
    "   - **Comparison with Adam -**\n",
    "\n",
    "     - **Learning Rate Adaptation :** Both RMSprop and Adam utilize adaptive learning rates. However, Adam also incorporates momentum, which gives it an edge in faster and more stable convergence, particularly in deep networks.\n",
    "   \n",
    "     - **Momentum :** Adam includes a momentum term that helps the optimization process gain speed in directions with persistent gradients, while RMSprop lacks this acceleration mechanism.\n",
    "   \n",
    "     - **General Performance :** Adam generally performs better than RMSprop in a wider variety of tasks due to its combination of momentum and adaptive learning rates. However, RMSprop can be more stable in problems with highly volatile or noisy gradients (e.g., in reinforcement learning).\n",
    "\n",
    "`In summary`, SGD is a fast and efficient optimizer suitable for large datasets but suffers from noisy updates. Adam improves upon this by combining momentum with adaptive learning rates, offering fast convergence but at the cost of potential overfitting. RMSprop, while simpler than Adam, effectively adapts learning rates but may struggle with convergence speed compared to Adam's momentum-boosted approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 3: Applying Optimizers**\n",
    "\n",
    "8. **Implement SGD, Adam, and RMSprop optimizers in a deep learning model using a framework of your choice. Train the model on a suitable dataset and compare their impact on model convergence and performance.**\n",
    "\n",
    "9. **Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural network architecture and task. Consider factors such as convergence speed, stability, and generalization performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans :-**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Implementation of SGD, Adam, and RMSprop in a Deep Learning Model**\n",
    "\n",
    "Let's implement and compare the performance of **SGD**, **Adam**, and **RMSprop** optimizers in a deep learning model using **PyTorch**. We'll use the **MNIST dataset** for digit classification, which is a common benchmark for neural networks.\n",
    "\n",
    "**`Step-by-Step Implementation` :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SGD:\n",
      "Epoch [1/10], Loss: 1.7068\n",
      "Epoch [2/10], Loss: 1.1281\n",
      "Epoch [3/10], Loss: 0.6775\n",
      "Epoch [4/10], Loss: 0.7355\n",
      "Epoch [5/10], Loss: 0.4882\n",
      "Epoch [6/10], Loss: 0.5206\n",
      "Epoch [7/10], Loss: 0.4404\n",
      "Epoch [8/10], Loss: 0.3873\n",
      "Epoch [9/10], Loss: 0.3783\n",
      "Epoch [10/10], Loss: 0.5893\n",
      "SGD Test Accuracy: 89.49%\n",
      "--------------------------------------------------\n",
      "Using Adam:\n",
      "Epoch [1/10], Loss: 0.1241\n",
      "Epoch [2/10], Loss: 0.1001\n",
      "Epoch [3/10], Loss: 0.1499\n",
      "Epoch [4/10], Loss: 0.1175\n",
      "Epoch [5/10], Loss: 0.1898\n",
      "Epoch [6/10], Loss: 0.0165\n",
      "Epoch [7/10], Loss: 0.1754\n",
      "Epoch [8/10], Loss: 0.0944\n",
      "Epoch [9/10], Loss: 0.0298\n",
      "Epoch [10/10], Loss: 0.0182\n",
      "Adam Test Accuracy: 97.43%\n",
      "--------------------------------------------------\n",
      "Using RMSprop:\n",
      "Epoch [1/10], Loss: 0.4762\n",
      "Epoch [2/10], Loss: 0.4324\n",
      "Epoch [3/10], Loss: 0.0663\n",
      "Epoch [4/10], Loss: 0.1096\n",
      "Epoch [5/10], Loss: 0.1586\n",
      "Epoch [6/10], Loss: 0.0043\n",
      "Epoch [7/10], Loss: 0.0180\n",
      "Epoch [8/10], Loss: 0.0797\n",
      "Epoch [9/10], Loss: 0.0450\n",
      "Epoch [10/10], Loss: 0.1526\n",
      "RMSprop Test Accuracy: 95.88%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 28 * 28  # MNIST images are 28x28\n",
    "hidden_size = 128\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Neural network model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Different optimizers to compare\n",
    "optimizers = {\n",
    "    'SGD': optim.SGD(model.parameters(), lr=learning_rate),\n",
    "    'Adam': optim.Adam(model.parameters(), lr=learning_rate),\n",
    "    'RMSprop': optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "}\n",
    "\n",
    "# Function to train and test the model\n",
    "def train_and_evaluate(optimizer_name, optimizer):\n",
    "    def reset_parameters(m):\n",
    "        if hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()\n",
    "\n",
    "    model.apply(reset_parameters)  # Reset model weights\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Evaluate the model on test data\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'{optimizer_name} Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Compare the optimizers\n",
    "for optimizer_name, optimizer in optimizers.items():\n",
    "    print(f'Using {optimizer_name}:')\n",
    "    train_and_evaluate(optimizer_name, optimizer)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **Considerations and Tradeoffs When Choosing an Optimizer** \n",
    "\n",
    "      -   Choosing the appropriate optimizer for a neural network depends on several factors related to the network architecture, the nature of the task, and performance considerations:\n",
    "\n",
    "            1. **Convergence Speed:**\n",
    "               - **Adam:** Known for its fast convergence, Adam is generally preferred when quick results are desired, especially for deeper and more complex models. It can handle noisy gradients and non-stationary objectives well.\n",
    "               - **SGD:** With a fixed learning rate, SGD converges more slowly compared to Adam. However, with proper tuning (e.g., learning rate schedules and momentum), SGD can perform well for tasks like image classification with CNNs.\n",
    "               - **RMSprop:** Offers a middle ground with reasonably fast convergence. It adapts learning rates based on recent gradient information, making it effective in scenarios with non-stationary data (e.g., reinforcement learning).\n",
    "\n",
    "            2. **Stability:**\n",
    "               - **SGD:** Without momentum, SGD might oscillate and show less stable convergence, especially in complex landscapes. Adding momentum can improve stability and accelerate convergence in deeper networks.\n",
    "               - **Adam:** Provides more stable convergence due to its adaptive learning rate and momentum. It's robust across a wide range of tasks but might require careful tuning to avoid overfitting.\n",
    "               - **RMSprop:** Offers stable updates by adjusting learning rates dynamically. However, it lacks the momentum component of Adam, which can help escape from plateaus and local minima more effectively.\n",
    "\n",
    "            3. **Generalization Performance:**\n",
    "               - **SGD:** In some cases, SGD can generalize better than adaptive optimizers like Adam, particularly when trained with learning rate annealing or momentum. It encourages the model to explore a wider range of minima.\n",
    "               - **Adam:** While Adam excels in convergence speed, it can sometimes overfit or fail to find the global minimum, leading to poorer generalization performance in some tasks.\n",
    "               - **RMSprop:** RMSprop can generalize well, especially in tasks with sparse or highly variable data, but it shares Adam’s susceptibility to local minima and overfitting.\n",
    "\n",
    "            4. **Task and Architecture Suitability:**\n",
    "               - **SGD:** Ideal for tasks with large datasets and relatively simpler architectures, such as shallow networks and traditional image classification. It benefits from strategies like learning rate scheduling and momentum.\n",
    "               - **Adam:** Works well in tasks with noisy gradients or complex architectures (e.g., deep networks, RNNs, transformers). It is often a good starting choice for a wide range of tasks due to its speed and ease of use.\n",
    "               - **RMSprop:** Particularly effective in tasks where the data is non-stationary, such as in reinforcement learning or when working with sequential data like time series.\n",
    "\n",
    "            5. **Memory and Computational Requirements:**\n",
    "               - **SGD:** Requires less memory compared to Adam and RMSprop, as it does not store extra information like momentum or squared gradients.\n",
    "               - **Adam:** Requires more memory due to storing moving averages of both first and second moments. This can be a limiting factor in memory-constrained environments.\n",
    "               - **RMSprop:** Similar to Adam in memory requirements but slightly less computationally expensive since it does not require maintaining the momentum term.\n",
    "\n",
    "**`Conclusion` :**\n",
    "\n",
    "- **SGD** is favored for simple tasks with large datasets where generalization is critical, especially when combined with momentum and \n",
    "learning rate scheduling.\n",
    "\n",
    "- **Adam** is the default choice for complex networks and tasks requiring fast convergence and robustness to noisy gradients.\n",
    "\n",
    "- **RMSprop** is a good alternative for non-stationary tasks or when stability is prioritized over speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
